import json
import csv
import os
from datetime import datetime
from typing import List, Dict, Any

# ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (expected_intent -> expected_intent_typeë¡œ ìˆ˜ì •)
manufacturing_dataset = [
    # ANOMALY_CHECK ì¼€ì´ìŠ¤ (30ê±´)
    {"id": 1, "query": "3ë²ˆ ì—£ì¹­ ì¥ë¹„ ì••ë ¥ì´ ì´ìƒí•´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 2, "query": "CVD ì¥ë¹„ ì˜¨ë„ê°€ ë„ˆë¬´ ë†’ì€ ê²ƒ ê°™ì•„ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 3, "query": "5ë²ˆ ì¥ë¹„ì—ì„œ ì´ìƒí•œ ì†Œë¦¬ê°€ ë‚˜ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 4, "query": "í”Œë¼ì¦ˆë§ˆ ì±”ë²„ ì••ë ¥ ì„¼ì„œ ìƒíƒœ í™•ì¸í•´ì£¼ì„¸ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 5, "query": "ì›¨ì´í¼ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 6, "query": "ì¦ì°© ì¥ë¹„ ì§„ê³µë„ê°€ í‰ì†Œì™€ ë‹¬ë¼ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 7, "query": "2ë²ˆ ë¼ì¸ ìƒì‚°ì´ ê°‘ìê¸° ë©ˆì·„ì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 8, "query": "ì‹ê° ê³µì •ì—ì„œ ë¶ˆëŸ‰ë¥ ì´ ê¸‰ì¦í–ˆìŠµë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 9, "query": "ì±”ë²„ ë‚´ë¶€ ì˜¨ë„ ì„¼ì„œê°€ ì˜¤ì‘ë™í•˜ëŠ” ê²ƒ ê°™ì•„ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 10, "query": "ê°€ìŠ¤ ìœ ëŸ‰ê³„ ìˆ˜ì¹˜ê°€ ë¶ˆì•ˆì •í•´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 11, "query": "RF íŒŒì›Œ ê³µê¸‰ì´ ê°„í—ì ìœ¼ë¡œ ëŠì–´ì ¸ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 12, "query": "ë¡œë“œë½ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 13, "query": "ì „ì²´ ì¥ë¹„ ìƒíƒœë¥¼ ì ê²€í•´ì£¼ì„¸ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 14, "query": "ëƒ‰ê°ìˆ˜ ì˜¨ë„ê°€ ì„¤ì •ê°’ì„ ë²—ì–´ë‚¬ì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 15, "query": "ì§„ê³µ íŒí”„ì—ì„œ ì´ìƒ ì§„ë™ì´ ê°ì§€ë©ë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 16, "query": "ì›¨ì´í¼ ì´ì†¡ ë¡œë´‡ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 17, "query": "í”Œë¼ì¦ˆë§ˆ ì í™”ê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 18, "query": "ë§ˆìŠ¤í¬ ì •ë ¬ ì‹œìŠ¤í…œ ì˜¤ì°¨ê°€ ì»¤ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 19, "query": "í™”í•™ ê¸°ìƒ ì¦ì°© ì¤‘ ê°€ìŠ¤ ëˆ„ì¶œì´ ì˜ì‹¬ë©ë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 20, "query": "ìŠ¤í¼í„°ë§ íƒ€ê²Ÿ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 21, "query": "ì´ì˜¨ ì£¼ì… ì¥ë¹„ ë¹” ì „ë¥˜ê°€ ë¶ˆì•ˆì •í•´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 22, "query": "í¬í† ë¦¬ì†Œê·¸ë˜í”¼ ë…¸ê´‘ ê°•ë„ê°€ ì´ìƒí•´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 23, "query": "CMP ì¥ë¹„ íŒ¨ë“œ ì••ë ¥ì´ ë¶ˆê· ë“±í•©ë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 24, "query": "ë°°ê¸° ì‹œìŠ¤í…œ íš¨ìœ¨ì´ ë–¨ì–´ì§„ ê²ƒ ê°™ì•„ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 25, "query": "ì›¨ì´í¼ ì¹´ì„¸íŠ¸ ë¡œë”©ì— ë¬¸ì œê°€ ìˆì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "MEDIUM"},
    {"id": 26, "query": "í”Œë¼ì¦ˆë§ˆ ë°€ë„ ë¶„í¬ê°€ ë¶ˆê· ì¼í•´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 27, "query": "ë°˜ì‘ì„± ì´ì˜¨ ì‹ê° ì†ë„ê°€ ëŠë ¤ì¡Œì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 28, "query": "ì—´ì²˜ë¦¬ë¡œ ì˜¨ë„ í”„ë¡œíŒŒì¼ì´ ì´ìƒí•©ë‹ˆë‹¤", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 29, "query": "í´ë¦°ë£¸ ì…ì ë†ë„ê°€ ê¸°ì¤€ì¹˜ë¥¼ ì´ˆê³¼í–ˆì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},
    {"id": 30, "query": "ì¥ë¹„ ì•ŒëŒì´ ê³„ì† ìš¸ë¦¬ê³  ìˆì–´ìš”", "expected_intent_type": "ANOMALY_CHECK", "expected_priority": "HIGH"},

    # OPTIMIZATION ì¼€ì´ìŠ¤ (25ê±´)
    {"id": 31, "query": "ìƒì‚° ìˆ˜ìœ¨ì„ ê°œì„ í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 32, "query": "ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 33, "query": "ê³µì • ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆì„ê¹Œìš”?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 34, "query": "ì›¨ì´í¼ ì²˜ë¦¬ëŸ‰ì„ ì¦ê°€ì‹œí‚¤ê³  ì‹¶ìŠµë‹ˆë‹¤", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 35, "query": "ë¶ˆëŸ‰ë¥ ì„ ì¤„ì´ëŠ” ìµœì í™” ë°©ì•ˆì„ ì œì•ˆí•´ì£¼ì„¸ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 36, "query": "ì±”ë²„ ì²­ì†Œ ì£¼ê¸°ë¥¼ ìµœì í™”í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 37, "query": "ê°€ìŠ¤ ì‚¬ìš©ëŸ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ë ¤ë©´?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 38, "query": "ì¥ë¹„ ê°€ë™ë¥ ì„ ë†’ì´ëŠ” ë°©ë²•ì„ ì°¾ì•„ì£¼ì„¸ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 39, "query": "ì‹ê° ê· ì¼ë„ë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‚˜ìš”?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 40, "query": "ì „ë ¥ ì†Œë¹„ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ì„±ëŠ¥ì€ ìœ ì§€í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 41, "query": "ë ˆì‹œí”¼ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 42, "query": "ì¥ë¹„ í™œìš©ë„ë¥¼ ê·¹ëŒ€í™”í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 43, "query": "ê³µì • ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 44, "query": "ì›¨ì´í¼ í’ˆì§ˆ ì¼ê´€ì„±ì„ ë†’ì´ë ¤ë©´?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 45, "query": "ë°˜ë„ì²´ ì œì¡° ë¹„ìš©ì„ ì ˆê°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 46, "query": "ê³µê¸‰ë§ íš¨ìœ¨ì„±ì„ ê°œì„ í•  ë°©ë²•ì€?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 47, "query": "ì¥ë¹„ ì˜ˆë°© ë³´ì „ ê³„íšì„ ìµœì í™”í•´ì£¼ì„¸ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 48, "query": "ìƒì‚° ìŠ¤ì¼€ì¤„ë§ì„ ê°œì„ í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 49, "query": "ì›ìì¬ ì¬ê³ ë¥¼ ìµœì í™”í•˜ë ¤ë©´?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 50, "query": "ì¸ë ¥ ë°°ì¹˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 51, "query": "ì„¤ë¹„ íˆ¬ì ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 52, "query": "ì œí’ˆ í’ˆì§ˆ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 53, "query": "ìƒì‚° ë¼ì¸ ë°¸ëŸ°ì‹±ì„ ìµœì í™”í•˜ë ¤ë©´?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 54, "query": "í™˜ê²½ ì˜í–¥ì„ ìµœì†Œí™”í•˜ë©´ì„œ ìƒì‚°ì„±ì„ ë†’ì´ë ¤ë©´?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},
    {"id": 55, "query": "ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ êµ¬í˜„ì„ ìœ„í•œ ê°œì„  ë°©ì•ˆì€?", "expected_intent_type": "OPTIMIZATION", "expected_priority": "MEDIUM"},

    # CONTROL ì¼€ì´ìŠ¤ (15ê±´)
    {"id": 56, "query": "CVD ì˜¨ë„ë¥¼ 350ë„ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 57, "query": "ì••ë ¥ì„ 100mTorrë¡œ ì¡°ì •í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 58, "query": "ê°€ìŠ¤ í”Œë¡œìš°ë¥¼ 50sccmìœ¼ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 59, "query": "ì¥ë¹„ë¥¼ ê¸´ê¸‰ ì •ì§€ì‹œì¼œì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "HIGH"},
    {"id": 60, "query": "RF íŒŒì›Œë¥¼ 2000Wë¡œ ì˜¬ë ¤ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 61, "query": "íŒí”„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 62, "query": "ìë™ ëª¨ë“œë¡œ ì „í™˜í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 63, "query": "ë ˆì‹œí”¼ Aë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 64, "query": "ì›¨ì´í¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 65, "query": "ì±”ë²„ í¼ì§€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 66, "query": "ì§„ê³µ ë°°ê¸°ë¥¼ ì¤‘ì§€í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 67, "query": "íˆí„°ë¥¼ ì¼œì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 68, "query": "ê³µì •ì„ ì¼ì‹œì •ì§€í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},
    {"id": 69, "query": "ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "HIGH"},
    {"id": 70, "query": "ì‹œìŠ¤í…œì„ ë¦¬ì…‹í•´ì£¼ì„¸ìš”", "expected_intent_type": "CONTROL", "expected_priority": "MEDIUM"},

    # PREDICTION ì¼€ì´ìŠ¤ (15ê±´)
    {"id": 71, "query": "ë‹¤ìŒ ì£¼ ìƒì‚°ëŸ‰ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 72, "query": "ì¥ë¹„ êµì²´ ì‹œê¸°ê°€ ì–¸ì œì¯¤ ë ê¹Œìš”?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 73, "query": "ìˆ˜ìœ¨ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ ì•Œê³  ì‹¶ì–´ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 74, "query": "í–¥í›„ 6ê°œì›”ê°„ ìœ ì§€ë³´ìˆ˜ ë¹„ìš©ì€?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 75, "query": "ì´ ì„¤ì •ìœ¼ë¡œ ì›¨ì´í¼ í’ˆì§ˆì´ ì–´ë–»ê²Œ ë ê¹Œìš”?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 76, "query": "ì—°ë§ê¹Œì§€ ì¥ë¹„ ê°€ë™ë¥  ì „ë§ì€?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 77, "query": "ê³µì • ë³€ê²½ ì‹œ ìˆ˜ìœ¨ ë³€í™”ë¥¼ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 78, "query": "ë‚´ì¼ ìƒì‚° ê³„íšì— ë§ëŠ” ì²˜ë¦¬ëŸ‰ ì˜ˆìƒì€?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 79, "query": "ì¥ë¹„ ê³ ì¥ ìœ„í—˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 80, "query": "ë‹¤ìŒ ë¶„ê¸° ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ì´ í•„ìš”í•´ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 81, "query": "ì›¨ì´í¼ ë¶ˆëŸ‰ë¥  ì¶”ì´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 82, "query": "ì‹ ê·œ ë ˆì‹œí”¼ ì ìš© ì‹œ ê²°ê³¼ ì˜ˆìƒì¹˜ëŠ”?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 83, "query": "ì¥ë¹„ ì„±ëŠ¥ ì €í•˜ ì‹œì ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 84, "query": "ê³µê¸‰ë§ ì°¨ì§ˆì´ ìƒì‚°ì— ë¯¸ì¹  ì˜í–¥ì€?", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},
    {"id": 85, "query": "ì‹œì¥ ìˆ˜ìš”ì— ë”°ë¥¸ ìƒì‚°ëŸ‰ ì¡°ì • ë¶„ì„", "expected_intent_type": "PREDICTION", "expected_priority": "MEDIUM"},

    # INFORMATION ì¼€ì´ìŠ¤ (15ê±´)
    {"id": 86, "query": "í˜„ì¬ ìƒì‚°ëŸ‰ì´ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 87, "query": "ì˜¤ëŠ˜ ìˆ˜ìœ¨ í˜„í™©ì„ ì•Œë ¤ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 88, "query": "ì¥ë¹„ ê°€ë™ë¥  í†µê³„ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 89, "query": "í˜„ì¬ ì›¨ì´í¼ ì¬ê³ ëŸ‰ì€ ì–¼ë§ˆì¸ê°€ìš”?", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 90, "query": "ì´ë²ˆ ì£¼ ìƒì‚° ì‹¤ì ì„ ì•Œë ¤ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 91, "query": "ì¥ë¹„ë³„ ì²˜ë¦¬ëŸ‰ ë°ì´í„°ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 92, "query": "í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 93, "query": "ê³µì • íŒŒë¼ë¯¸í„° ì„¤ì •ê°’ì„ ì•Œë ¤ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 94, "query": "ìœ ì§€ë³´ìˆ˜ ê¸°ë¡ì„ ì¡°íšŒí•´ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 95, "query": "ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ í˜„í™©ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 96, "query": "í˜„ì¬ ê°€ë™ ì¤‘ì¸ ì¥ë¹„ ëª©ë¡ì„ ì•Œë ¤ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 97, "query": "ê³µì •ë³„ ì†Œìš”ì‹œê°„ ë°ì´í„°ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 98, "query": "ì›ìì¬ ì†Œëª¨ëŸ‰ í†µê³„ê°€ ê¶ê¸ˆí•´ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 99, "query": "ë¶ˆëŸ‰í’ˆ ë°œìƒ í˜„í™©ì„ ì•Œë ¤ì£¼ì„¸ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"},
    {"id": 100, "query": "í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì‹¶ì–´ìš”", "expected_intent_type": "INFORMATION", "expected_priority": "LOW"}
]

class ManufacturingDatasetManager:
    """ì œì¡°ì—… AI ë°ì´í„°ì…‹ ê´€ë¦¬ì"""
    
    def __init__(self, base_dir: str = "data"):
        self.base_dir = base_dir
        self.ensure_directories()
    
    def ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.base_dir, exist_ok=True)
        os.makedirs(os.path.join(self.base_dir, "backups"), exist_ok=True)
        print(f"âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ ì¤€ë¹„: {self.base_dir}")
    
    def save_as_json(self, filename: str = "manufacturing_dataset_100.json") -> str:
        """JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ê¶Œì¥)"""
        filepath = os.path.join(self.base_dir, filename)
        
        # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
        data_with_metadata = {
            "metadata": {
                "title": "ì œì¡°ì—… AI ì—ì´ì „íŠ¸ ëª…ë ¹ì–´ ë¶„ë¥˜ ë°ì´í„°ì…‹",
                "description": "ì œì¡°ì—… í˜„ì¥ì˜ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ì˜ë„ë³„ë¡œ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹",
                "created_at": datetime.now().isoformat(),
                "total_samples": len(manufacturing_dataset),
                "format_version": "1.0",
                "intent_types": {
                    "ANOMALY_CHECK": "ì´ìƒ ê°ì§€ ë° ì ê²€",
                    "OPTIMIZATION": "ìµœì í™” ë° ê°œì„ ",
                    "CONTROL": "ì œì–´ ë° ì¡°ì‘",
                    "PREDICTION": "ì˜ˆì¸¡ ë° ë¶„ì„",
                    "INFORMATION": "ì •ë³´ ì¡°íšŒ"
                },
                "priority_levels": {
                    "HIGH": "ê¸´ê¸‰/ì¤‘ìš”",
                    "MEDIUM": "ë³´í†µ",
                    "LOW": "ë‚®ìŒ"
                }
            },
            "dataset": manufacturing_dataset
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_with_metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSON ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def save_as_csv(self, filename: str = "manufacturing_dataset_100.csv") -> str:
        """CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ (Excel í˜¸í™˜)"""
        filepath = os.path.join(self.base_dir, filename)
        
        fieldnames = ['id', 'query', 'expected_intent_type', 'expected_priority']
        
        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(manufacturing_dataset)
        
        print(f"âœ… CSV ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def save_as_excel(self, filename: str = "manufacturing_dataset_100.xlsx") -> str:
        """Excel í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ë¶„ì„ìš©)"""
        try:
            import pandas as pd
            
            filepath = os.path.join(self.base_dir, filename)
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(manufacturing_dataset)
            
            # Excel íŒŒì¼ë¡œ ì €ì¥ (ë‹¤ì¤‘ ì‹œíŠ¸)
            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
                # ì „ì²´ ë°ì´í„°
                df.to_excel(writer, sheet_name='ì „ì²´ë°ì´í„°', index=False)
                
                # ì˜ë„ë³„ ë¶„ë¦¬
                for intent in df['expected_intent_type'].unique():
                    intent_df = df[df['expected_intent_type'] == intent]
                    sheet_name = intent.replace('_', ' ')[:31]
                    intent_df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # í†µê³„ ì •ë³´
                stats_data = []
                for intent in df['expected_intent_type'].unique():
                    count = len(df[df['expected_intent_type'] == intent])
                    percentage = count / len(df) * 100
                    stats_data.append({
                        'ì˜ë„ ìœ í˜•': intent,
                        'ë°ì´í„° ìˆ˜': count,
                        'ë¹„ìœ¨(%)': round(percentage, 1)
                    })
                
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='í†µê³„', index=False)
            
            print(f"âœ… Excel ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except ImportError:
            print("âŒ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pandas openpyxl")
            return ""
    
    def backup_dataset(self) -> str:
        """ë°ì´í„°ì…‹ ë°±ì—… ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"manufacturing_dataset_backup_{timestamp}.json"
        backup_path = os.path.join(self.base_dir, "backups", backup_filename)
        
        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(manufacturing_dataset, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_path}")
        return backup_path
    
    def load_dataset(self, filename: str = "manufacturing_dataset_100.json") -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ë°ì´í„°ì…‹ ë¡œë“œ"""
        filepath = os.path.join(self.base_dir, filename)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ë°ì´í„°ì…‹ë§Œ ì¶”ì¶œ
            if isinstance(data, dict) and "dataset" in data:
                dataset = data["dataset"]
                print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê±´")
                return dataset
            else:
                print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(data)}ê±´")
                return data
                
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return []
        except json.JSONDecodeError:
            print(f"âŒ JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {filepath}")
            return []
    
    def analyze_dataset(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ë¶„ì„"""
        intent_counts = {}
        priority_counts = {}
        
        for item in manufacturing_dataset:
            intent = item['expected_intent_type']
            priority = item['expected_priority']
            
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
            priority_counts[priority] = priority_counts.get(priority, 0) + 1
        
        analysis = {
            "total_samples": len(manufacturing_dataset),
            "intent_distribution": intent_counts,
            "priority_distribution": priority_counts,
            "balance_score": min(intent_counts.values()) / max(intent_counts.values())
        }
        
        print("ğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„ ê²°ê³¼:")
        print(f"ì´ ë°ì´í„° ìˆ˜: {analysis['total_samples']}ê±´")
        print("\nì˜ë„ë³„ ë¶„í¬:")
        for intent, count in intent_counts.items():
            percentage = count / len(manufacturing_dataset) * 100
            print(f"  {intent}: {count}ê±´ ({percentage:.1f}%)")
        
        print("\nìš°ì„ ìˆœìœ„ë³„ ë¶„í¬:")
        for priority, count in priority_counts.items():
            percentage = count / len(manufacturing_dataset) * 100
            print(f"  {priority}: {count}ê±´ ({percentage:.1f}%)")
        
        print(f"\në°ì´í„° ê· í˜•ë„: {analysis['balance_score']:.2f} (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê· í˜•)")
        
        return analysis
    
    def save_all_formats(self) -> Dict[str, str]:
        """ëª¨ë“  í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        results = {}
        
        print("ğŸ’¾ ëª¨ë“  í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
        
        # JSON ì €ì¥
        results['json'] = self.save_as_json()
        
        # CSV ì €ì¥
        results['csv'] = self.save_as_csv()
        
        # Excel ì €ì¥ (ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆëŠ” ê²½ìš°)
        excel_path = self.save_as_excel()
        if excel_path:
            results['excel'] = excel_path
        
        # ë°±ì—… ìƒì„±
        results['backup'] = self.backup_dataset()
        
        print(f"âœ… ëª¨ë“  í˜•ì‹ ì €ì¥ ì™„ë£Œ! ì´ {len(results)}ê°œ íŒŒì¼")
        return results

def quick_save():
    """ë¹ ë¥¸ ì €ì¥ ì‹¤í–‰"""
    manager = ManufacturingDatasetManager()
    
    # ë°ì´í„°ì…‹ ë¶„ì„
    manager.analyze_dataset()
    
    # ëª¨ë“  í˜•ì‹ìœ¼ë¡œ ì €ì¥
    saved_files = manager.save_all_formats()
    
    print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
    for format_type, filepath in saved_files.items():
        print(f"  {format_type.upper()}: {filepath}")
    
    print(f"\nğŸ¯ ì‚¬ìš© ê¶Œì¥ì‚¬í•­:")
    print(f"  - í”„ë¡œê·¸ë˜ë° ì‘ì—…: manufacturing_dataset_100.json")
    print(f"  - Excelì—ì„œ í™•ì¸: manufacturing_dataset_100.csv")
    print(f"  - ë°ì´í„° ë¶„ì„: manufacturing_dataset_100.xlsx")
    
    return saved_files

def create_sample_usage_script():
    """ë°ì´í„°ì…‹ ì‚¬ìš© ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    
    usage_script = '''#!/usr/bin/env python3
"""
ì œì¡°ì—… AI ë°ì´í„°ì…‹ ì‚¬ìš© ì˜ˆì œ
"""
import json
import os

def load_and_use_dataset():
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì‚¬ìš© ì˜ˆì œ"""
    
    # 1. JSON ë°ì´í„°ì…‹ ë¡œë“œ
    with open('data/manufacturing_dataset_100.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë©”íƒ€ë°ì´í„°ì™€ ë°ì´í„°ì…‹ ë¶„ë¦¬
    metadata = data.get('metadata', {})
    dataset = data.get('dataset', [])
    
    print(f"ë°ì´í„°ì…‹ ì •ë³´:")
    print(f"  ì œëª©: {metadata.get('title', 'Unknown')}")
    print(f"  ì´ ë°ì´í„° ìˆ˜: {len(dataset)}ê±´")
    print(f"  ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}")
    
    # 2. ì˜ë„ë³„ ë°ì´í„° ë¶„ë¥˜
    intent_groups = {}
    for item in dataset:
        intent = item['expected_intent_type']
        if intent not in intent_groups:
            intent_groups[intent] = []
        intent_groups[intent].append(item)
    
    # 3. ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
    print(f"\\nê° ì˜ë„ë³„ ìƒ˜í”Œ ë°ì´í„°:")
    for intent, items in intent_groups.items():
        print(f"\\n{intent} ({len(items)}ê±´):")
        for item in items[:3]:  # ê° ì˜ë„ë³„ë¡œ 3ê°œì”©ë§Œ ì¶œë ¥
            print(f"  ID {item['id']}: {item['query']}")
            print(f"    â†’ Priority: {item['expected_priority']}")

if __name__ == "__main__":
    load_and_use_dataset()
'''
    
    script_path = "data/usage_example.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(usage_script)
    
    print(f"âœ… ì‚¬ìš© ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {script_path}")
    return script_path

def validate_dataset_format():
    """ë°ì´í„°ì…‹ í˜•ì‹ ê²€ì¦"""
    
    print("ğŸ” ë°ì´í„°ì…‹ í˜•ì‹ ê²€ì¦ ì¤‘...")
    
    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
    required_fields = ['id', 'query', 'expected_intent_type', 'expected_priority']
    valid_intents = ['ANOMALY_CHECK', 'OPTIMIZATION', 'CONTROL', 'PREDICTION', 'INFORMATION']
    valid_priorities = ['HIGH', 'MEDIUM', 'LOW']
    
    errors = []
    warnings = []
    
    for i, item in enumerate(manufacturing_dataset):
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        for field in required_fields:
            if field not in item:
                errors.append(f"ID {item.get('id', i+1)}: í•„ìˆ˜ í•„ë“œ '{field}' ëˆ„ë½")
        
        # ì˜ë„ ìœ í˜• ê²€ì¦
        if 'expected_intent_type' in item:
            if item['expected_intent_type'] not in valid_intents:
                errors.append(f"ID {item['id']}: ì˜ëª»ëœ ì˜ë„ ìœ í˜• '{item['expected_intent_type']}'")
        
        # ìš°ì„ ìˆœìœ„ ê²€ì¦
        if 'expected_priority' in item:
            if item['expected_priority'] not in valid_priorities:
                errors.append(f"ID {item['id']}: ì˜ëª»ëœ ìš°ì„ ìˆœìœ„ '{item['expected_priority']}'")
        
        # ì¿¼ë¦¬ ê¸¸ì´ ê²€ì¦
        if 'query' in item:
            if len(item['query']) < 5:
                warnings.append(f"ID {item['id']}: ì¿¼ë¦¬ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (5ì ë¯¸ë§Œ)")
            elif len(item['query']) > 200:
                warnings.append(f"ID {item['id']}: ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (200ì ì´ˆê³¼)")
    
    # ID ì¤‘ë³µ ê²€ì¦
    ids = [item['id'] for item in manufacturing_dataset]
    if len(ids) != len(set(ids)):
        errors.append("ID ì¤‘ë³µì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # ê²°ê³¼ ì¶œë ¥
    if errors:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {len(errors)}ê°œ ì˜¤ë¥˜ ë°œê²¬")
        for error in errors[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"  - {error}")
        if len(errors) > 5:
            print(f"  ... ë° {len(errors)-5}ê°œ ë”")
    else:
        print("âœ… ë°ì´í„°ì…‹ í˜•ì‹ ê²€ì¦ í†µê³¼")
    
    if warnings:
        print(f"âš ï¸  {len(warnings)}ê°œ ê²½ê³ :")
        for warning in warnings[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"  - {warning}")
        if len(warnings) > 3:
            print(f"  ... ë° {len(warnings)-3}ê°œ ë”")
    
    return len(errors) == 0

def create_evaluation_compatible_format():
    """í‰ê°€ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    # ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„±ì„ ìœ„í•´ í•„ë“œëª… ë³€ê²½
    compatible_dataset = []
    
    for item in manufacturing_dataset:
        compatible_item = {
            "id": item["id"],
            "query": item["query"],
            "expected_intent": item["expected_intent_type"],  # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜
            "expected_priority": item["expected_priority"]
        }
        compatible_dataset.append(compatible_item)
    
    # í˜¸í™˜ ë²„ì „ ì €ì¥
    compat_path = os.path.join("data", "compatible_dataset_100.json")
    with open(compat_path, 'w', encoding='utf-8') as f:
        json.dump(compatible_dataset, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í˜¸í™˜ í˜•ì‹ ë°ì´í„°ì…‹ ìƒì„±: {compat_path}")
    return compat_path

def generate_readme():
    """README íŒŒì¼ ìƒì„±"""
    
    readme_content = """# ì œì¡°ì—… AI ì—ì´ì „íŠ¸ ëª…ë ¹ì–´ ë¶„ë¥˜ ë°ì´í„°ì…‹

## ê°œìš”
ë°˜ë„ì²´ ì œì¡°ì—… í˜„ì¥ì—ì„œ ë°œìƒí•˜ëŠ” ìì—°ì–´ ì¿¼ë¦¬ë¥¼ 5ê°€ì§€ ì˜ë„ë¡œ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

## ë°ì´í„°ì…‹ êµ¬ì„±
- **ì´ ë°ì´í„° ìˆ˜**: 100ê±´
- **ì˜ë„ ìœ í˜•**: 5ê°œ (ANOMALY_CHECK, OPTIMIZATION, CONTROL, PREDICTION, INFORMATION)
- **ìš°ì„ ìˆœìœ„**: 3ê°œ (HIGH, MEDIUM, LOW)

### ì˜ë„ë³„ ë¶„í¬
- **ANOMALY_CHECK**: 30ê±´ (30%) - ì´ìƒ ê°ì§€ ë° ì ê²€
- **OPTIMIZATION**: 25ê±´ (25%) - ìµœì í™” ë° ê°œì„ 
- **CONTROL**: 15ê±´ (15%) - ì œì–´ ë° ì¡°ì‘
- **PREDICTION**: 15ê±´ (15%) - ì˜ˆì¸¡ ë° ë¶„ì„
- **INFORMATION**: 15ê±´ (15%) - ì •ë³´ ì¡°íšŒ

### ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬
- **HIGH**: ì£¼ë¡œ ANOMALY_CHECKì™€ ê¸´ê¸‰ CONTROL
- **MEDIUM**: OPTIMIZATION, PREDICTION, ì¼ë°˜ CONTROL
- **LOW**: ëŒ€ë¶€ë¶„ì˜ INFORMATION

## íŒŒì¼ í˜•ì‹

### 1. JSON í˜•ì‹ (ê¶Œì¥)
```
manufacturing_dataset_100.json
```
- ë©”íƒ€ë°ì´í„° í¬í•¨
- í”„ë¡œê·¸ë˜ë° ì‘ì—…ì— ìµœì 
- UTF-8 ì¸ì½”ë”©

### 2. CSV í˜•ì‹ (Excel í˜¸í™˜)
```
manufacturing_dataset_100.csv
```
- Excelì—ì„œ ë°”ë¡œ ì—´ëŒ ê°€ëŠ¥
- UTF-8 BOM ì¸ì½”ë”©

### 3. Excel í˜•ì‹ (ë¶„ì„ìš©)
```
manufacturing_dataset_100.xlsx
```
- ë‹¤ì¤‘ ì‹œíŠ¸ êµ¬ì¡°
- ì˜ë„ë³„ ë¶„ë¦¬ëœ ì‹œíŠ¸
- í†µê³„ ì •ë³´ ì‹œíŠ¸ í¬í•¨

## ë°ì´í„° êµ¬ì¡°

### JSON í˜•ì‹
```json
{
  "metadata": {
    "title": "ì œì¡°ì—… AI ì—ì´ì „íŠ¸ ëª…ë ¹ì–´ ë¶„ë¥˜ ë°ì´í„°ì…‹",
    "created_at": "2024-12-18T...",
    "total_samples": 100,
    "intent_types": {...},
    "priority_levels": {...}
  },
  "dataset": [
    {
      "id": 1,
      "query": "3ë²ˆ ì—£ì¹­ ì¥ë¹„ ì••ë ¥ì´ ì´ìƒí•´ìš”",
      "expected_intent_type": "ANOMALY_CHECK",
      "expected_priority": "HIGH"
    }
  ]
}
```

## ì‚¬ìš©ë²•

### 1. Pythonì—ì„œ ë¡œë“œ
```python
import json

with open('data/manufacturing_dataset_100.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

dataset = data['dataset']
```

### 2. í‰ê°€ ì‹œìŠ¤í…œê³¼ ì—°ë™
```python
from enhanced_instruction_rf import EnhancedInstructionRefinementClient
from evaluation_script import InstructionRFEvaluator

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = EnhancedInstructionRefinementClient()

# í‰ê°€ ì‹¤í–‰
evaluator = InstructionRFEvaluator(client, dataset)
metrics = evaluator.run_evaluation()
```

## ëª©í‘œ ì„±ëŠ¥
- **Intent ë¶„ë¥˜ ì •í™•ë„**: 99% ì´ìƒ
- **Priority ë¶„ë¥˜ ì •í™•ë„**: 99% ì´ìƒ
- **ì „ì²´ ì •í™•ë„**: 99% ì´ìƒ

## ë°ì´í„° ì˜ˆì‹œ

### ANOMALY_CHECK (ì´ìƒ ê°ì§€)
- "3ë²ˆ ì—£ì¹­ ì¥ë¹„ ì••ë ¥ì´ ì´ìƒí•´ìš”" (HIGH)
- "í”Œë¼ì¦ˆë§ˆ ì±”ë²„ ì••ë ¥ ì„¼ì„œ ìƒíƒœ í™•ì¸í•´ì£¼ì„¸ìš”" (MEDIUM)

### OPTIMIZATION (ìµœì í™”)
- "ìƒì‚° ìˆ˜ìœ¨ì„ ê°œì„ í•˜ê³  ì‹¶ì–´ìš”" (MEDIUM)
- "ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”" (MEDIUM)

### CONTROL (ì œì–´)
- "ì¥ë¹„ë¥¼ ê¸´ê¸‰ ì •ì§€ì‹œì¼œì£¼ì„¸ìš”" (HIGH)
- "CVD ì˜¨ë„ë¥¼ 350ë„ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”" (MEDIUM)

### PREDICTION (ì˜ˆì¸¡)
- "ë‹¤ìŒ ì£¼ ìƒì‚°ëŸ‰ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”" (MEDIUM)
- "ì¥ë¹„ êµì²´ ì‹œê¸°ê°€ ì–¸ì œì¯¤ ë ê¹Œìš”?" (MEDIUM)

### INFORMATION (ì •ë³´)
- "í˜„ì¬ ìƒì‚°ëŸ‰ì´ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?" (LOW)
- "ì˜¤ëŠ˜ ìˆ˜ìœ¨ í˜„í™©ì„ ì•Œë ¤ì£¼ì„¸ìš”" (LOW)

## ë²„ì „ ê´€ë¦¬
- v1.0: ì´ˆê¸° 100ê±´ ë°ì´í„°ì…‹
- ë°±ì—… íŒŒì¼: `data/backups/manufacturing_dataset_backup_YYYYMMDD_HHMMSS.json`

## ë¼ì´ì„ ìŠ¤
ë‚´ë¶€ ì—°êµ¬ìš© ë°ì´í„°ì…‹
"""
    
    readme_path = "data/README.md"
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… README íŒŒì¼ ìƒì„±: {readme_path}")
    return readme_path

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ì „ì²´ ë°ì´í„°ì…‹ ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
    
    print("ğŸš€ ì œì¡°ì—… AI ë°ì´í„°ì…‹ ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    print("="*50)
    
    # 1. ë°ì´í„°ì…‹ í˜•ì‹ ê²€ì¦
    if not validate_dataset_format():
        print("âŒ ë°ì´í„°ì…‹ í˜•ì‹ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    # 2. ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = ManufacturingDatasetManager()
    
    # 3. ë°ì´í„°ì…‹ ë¶„ì„
    analysis = manager.analyze_dataset()
    
    # 4. ëª¨ë“  í˜•ì‹ìœ¼ë¡œ ì €ì¥
    saved_files = manager.save_all_formats()
    
    # 5. í‰ê°€ ì‹œìŠ¤í…œ í˜¸í™˜ í˜•ì‹ ìƒì„±
    compat_path = create_evaluation_compatible_format()
    
    # 6. ì‚¬ìš© ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    example_path = create_sample_usage_script()
    
    # 7. README íŒŒì¼ ìƒì„±
    readme_path = generate_readme()
    
    print("\n" + "="*50)
    print("âœ… ë°ì´í„°ì…‹ ì €ì¥ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ìš”ì•½:")
    print(f"  ì´ ë°ì´í„°: {analysis['total_samples']}ê±´")
    print(f"  ì˜ë„ ìœ í˜•: {len(analysis['intent_distribution'])}ê°œ")
    print(f"  ë°ì´í„° ê· í˜•ë„: {analysis['balance_score']:.2f}")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for file_type, path in saved_files.items():
        print(f"  {file_type.upper()}: {path}")
    print(f"  í˜¸í™˜ì„±: {compat_path}")
    print(f"  ì˜ˆì œ: {example_path}")
    print(f"  ë¬¸ì„œ: {readme_path}")
    
    print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. ë°ì´í„° í™•ì¸: data/manufacturing_dataset_100.csv ì—´ì–´ë³´ê¸°")
    print(f"  2. ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸: python usage_example.py")
    print(f"  3. í‰ê°€ ì‹¤í–‰: enhanced_instruction_rf.py ì‚¬ìš©")

if __name__ == "__main__":
    # ë¹ ë¥¸ ì €ì¥ ì‹¤í–‰
    quick_save()
    
    print("\n" + "="*50)
    print("ìƒì„¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”:")
    print("python complete_dataset_with_storage.py")
    
    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    main()