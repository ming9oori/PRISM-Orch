#!/bin/bash
# vLLM ì„œë²„ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "=== vLLM High-Performance Inference Server Deployment ==="

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MODEL_NAME=${MODEL_NAME:-"microsoft/DialoGPT-large"}
export TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
export GPU_COUNT=${GPU_COUNT:-1}

# GPU í™•ì¸
echo "Checking GPU availability..."
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
echo "CUDA Version:"
nvcc --version

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo "Building Docker image..."
docker build -f docker/Dockerfile.vllm -t vllm-inference-server .

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "Cleaning up existing containers..."
docker-compose down

# ìƒˆ ì„œë¹„ìŠ¤ ì‹œì‘
echo "Starting services..."
docker-compose up -d

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
echo "Checking service status..."
sleep 30
docker-compose ps

# í—¬ìŠ¤ì²´í¬
echo "Health check..."
for i in {1..10}; do
    if curl -f http://localhost:8000/health; then
        echo "âœ… Server is healthy!"
        break
    else
        echo "â³ Waiting for server to be ready... ($i/10)"
        sleep 10
    fi
done

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
echo "Running performance test..."
curl -X POST http://localhost:8000/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "agent_type": "monitoring",
    "prompt": "ì„¼ì„œ ì˜¨ë„ê°€ 195ë„ì…ë‹ˆë‹¤. ì •ìƒ ë²”ìœ„ëŠ” 150-200ë„ì¸ë° ìƒí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
    "max_tokens": 256
  }'

echo ""
echo "âœ… Deployment complete!"
echo "ğŸ“Š Server URL: http://localhost:8000"
echo "ğŸ¥ Health Check: http://localhost:8000/health"
echo "ğŸ“ˆ Stats: http://localhost:8000/api/v1/stats"
echo "ğŸ” Grafana: http://localhost:3000 (admin/admin)" 