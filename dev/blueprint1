# docker-compose.yml
# vLLM ê³ ì„±ëŠ¥ ì¶”ë¡  ì„œë²„ Docker êµ¬ì„±

version: '3.8'

services:
  # vLLM ì¶”ë¡  ì„œë²„
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm_inference_server
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME=microsoft/DialoGPT-large
      - TENSOR_PARALLEL_SIZE=2
      - MAX_MODEL_LEN=4096
      - GPU_MEMORY_UTILIZATION=0.9
      - CUDA_VISIBLE_DEVICES=0,1  # ì‚¬ìš©í•  GPU ì§€ì •
    volumes:
      - model_cache:/root/.cache/huggingface
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # GPU ê°œìˆ˜
              capabilities: [gpu]
    shm_size: 16gb  # ê³µìœ  ë©”ëª¨ë¦¬ í¬ê¸° (ì¤‘ìš”!)

  # Redis (ìºì‹±ìš©)
  redis:
    image: redis:7-alpine
    container_name: vllm_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru

  # Nginx (ë¡œë“œ ë°¸ëŸ°ì„œ + ìºì‹±)
  nginx:
    image: nginx:alpine
    container_name: vllm_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - vllm-server
    restart: unless-stopped

  # ëª¨ë‹ˆí„°ë§ (Prometheus + Grafana)
  prometheus:
    image: prom/prometheus:latest
    container_name: vllm_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: vllm_grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    restart: unless-stopped

volumes:
  model_cache:
  redis_data:
  prometheus_data:
  grafana_data:

---
# Dockerfile.vllm
FROM nvidia/cuda:12.1-devel-ubuntu22.04

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements-vllm.txt .
RUN pip3 install --no-cache-dir -r requirements-vllm.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY vllm_inference_server.py .
COPY config/ ./config/

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p /app/logs

# ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ì„œë²„ ì‹¤í–‰
CMD ["python3", "vllm_inference_server.py", "--host", "0.0.0.0", "--port", "8000"]

---
# requirements-vllm.txt
vllm==0.3.2
fastapi==0.104.1
uvicorn[standard]==0.24.0
torch==2.1.0
transformers==4.36.0
accelerate==0.25.0
pydantic==2.5.0
psutil==5.9.6
gputil==1.4.0
numpy==1.24.3
aiohttp==3.9.1
redis==5.0.1

---
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream vllm_backend {
        server vllm-server:8000 max_fails=3 fail_timeout=30s;
        # ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€ ì‹œ
        # server vllm-server-2:8000 max_fails=3 fail_timeout=30s;
    }

    # ìºì‹± ì„¤ì •
    proxy_cache_path /tmp/nginx_cache levels=1:2 keys_zone=llm_cache:10m max_size=1g inactive=60m;

    server {
        listen 80;
        server_name localhost;

        # ì¼ë°˜ ìš”ì²­
        location / {
            proxy_pass http://vllm_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ê¸´ ì¶”ë¡  ì‹œê°„ ê³ ë ¤)
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
            
            # ë²„í¼ë§ ì„¤ì •
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # API ìš”ì²­ (ìºì‹± ì ìš©)
        location /api/v1/generate {
            proxy_pass http://vllm_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            
            # ìºì‹± (ë™ì¼í•œ ìš”ì²­ ê²°ê³¼ ì¬ì‚¬ìš©)
            proxy_cache llm_cache;
            proxy_cache_key "$request_method$request_uri$request_body";
            proxy_cache_valid 200 10m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            
            # ìºì‹œ í—¤ë” ì¶”ê°€
            add_header X-Cache-Status $upstream_cache_status;
            
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }

        # í—¬ìŠ¤ì²´í¬
        location /health {
            proxy_pass http://vllm_backend/health;
            proxy_connect_timeout 5s;
            proxy_send_timeout 5s;
            proxy_read_timeout 5s;
        }

        # ì •ì  íŒŒì¼ (ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë“±)
        location /static/ {
            alias /var/www/static/;
            expires 1d;
            add_header Cache-Control "public, immutable";
        }
    }
}

---
# config/models.yaml
# ì§€ì› ëª¨ë¸ ì„¤ì •

models:
  # ì†Œí˜• ëª¨ë¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
  small:
    name: "microsoft/DialoGPT-medium"
    tensor_parallel_size: 1
    max_model_len: 2048
    gpu_memory_utilization: 0.7
    description: "ê°œë°œ ë° í…ŒìŠ¤íŠ¸ìš© ì†Œí˜• ëª¨ë¸"

  # ì¤‘í˜• ëª¨ë¸ (ì¼ë°˜ ìš´ì˜ìš©)  
  medium:
    name: "microsoft/DialoGPT-large"
    tensor_parallel_size: 1
    max_model_len: 4096
    gpu_memory_utilization: 0.9
    description: "ì¼ë°˜ ìš´ì˜ìš© ì¤‘í˜• ëª¨ë¸"

  # ëŒ€í˜• ëª¨ë¸ (ê³ ì„±ëŠ¥ ìš”êµ¬ì‹œ)
  large:
    name: "meta-llama/Llama-2-13b-chat-hf"
    tensor_parallel_size: 2
    max_model_len: 4096
    gpu_memory_utilization: 0.95
    description: "ê³ ì„±ëŠ¥ ëŒ€í˜• ëª¨ë¸"

  # ì´ˆëŒ€í˜• ëª¨ë¸ (ìµœê³  ì„±ëŠ¥)
  xlarge:
    name: "meta-llama/Llama-2-70b-chat-hf"
    tensor_parallel_size: 4
    max_model_len: 4096
    gpu_memory_utilization: 0.95
    description: "ìµœê³  ì„±ëŠ¥ ì´ˆëŒ€í˜• ëª¨ë¸"

# ì—ì´ì „íŠ¸ë³„ ê¶Œì¥ ëª¨ë¸
agent_recommendations:
  orchestration: "medium"    # ë³µì¡í•œ ê³„íš ìˆ˜ë¦½
  monitoring: "small"        # ë¹ ë¥¸ ë¶„ì„ ì‘ë‹µ
  prediction: "large"        # ì •í™•í•œ ì˜ˆì¸¡ í•´ì„
  control: "medium"          # ì•ˆì „í•œ ì œì–´ ê²°ì •

---
# config/optimization.yaml
# ì„±ëŠ¥ ìµœì í™” ì„¤ì •

# vLLM ì—”ì§„ ìµœì í™”
engine_optimization:
  # ë©”ëª¨ë¦¬ ìµœì í™”
  gpu_memory_utilization: 0.9
  swap_space: 4  # GB
  cpu_offload_gb: 0
  
  # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
  max_num_seqs: 256
  max_num_batched_tokens: 8192
  max_paddings: 256
  
  # CUDA ìµœì í™”
  enforce_eager: false  # CUDA ê·¸ë˜í”„ ì‚¬ìš©
  use_v2_block_manager: true
  enable_prefix_caching: true
  
  # í† í° ìƒì„± ìµœì í™”
  disable_log_stats: true
  disable_log_requests: false

# ì‹œìŠ¤í…œ ìµœì í™”
system_optimization:
  # í”„ë¡œì„¸ìŠ¤ ì„¤ì •
  worker_use_ray: false
  max_parallel_loading_workers: 4
  
  # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
  served_model_name: null
  chat_template: null
  response_role: "assistant"
  
  # ë¡œê¹… ì„¤ì •
  log_level: "INFO"
  log_requests: true
  log_stats_interval: 10

# í•˜ë“œì›¨ì–´ë³„ ê¶Œì¥ ì„¤ì •
hardware_profiles:
  # ë‹¨ì¼ A100 80GB
  single_a100_80gb:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.95
    max_model_len: 8192
    max_num_seqs: 512
    
  # ë“€ì–¼ A100 80GB  
  dual_a100_80gb:
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.95
    max_model_len: 8192
    max_num_seqs: 1024
    
  # 4x A100 80GB
  quad_a100_80gb:
    tensor_parallel_size: 4
    gpu_memory_utilization: 0.95
    max_model_len: 8192
    max_num_seqs: 2048
    
  # RTX 4090 (ê°œë°œìš©)
  rtx_4090:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_model_len: 4096
    max_num_seqs: 128

---
# deploy.sh
#!/bin/bash
# vLLM ì„œë²„ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "=== vLLM High-Performance Inference Server Deployment ==="

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MODEL_NAME=${MODEL_NAME:-"microsoft/DialoGPT-large"}
export TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
export GPU_COUNT=${GPU_COUNT:-1}

# GPU í™•ì¸
echo "Checking GPU availability..."
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
echo "CUDA Version:"
nvcc --version

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo "Building Docker image..."
docker build -f Dockerfile.vllm -t vllm-inference-server .

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "Cleaning up existing containers..."
docker-compose down

# ìƒˆ ì„œë¹„ìŠ¤ ì‹œì‘
echo "Starting services..."
docker-compose up -d

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
echo "Checking service status..."
sleep 30
docker-compose ps

# í—¬ìŠ¤ì²´í¬
echo "Health check..."
for i in {1..10}; do
    if curl -f http://localhost:8000/health; then
        echo "âœ… Server is healthy!"
        break
    else
        echo "â³ Waiting for server to be ready... ($i/10)"
        sleep 10
    fi
done

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
echo "Running performance test..."
curl -X POST http://localhost:8000/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "agent_type": "monitoring",
    "prompt": "ì„¼ì„œ ì˜¨ë„ê°€ 195ë„ì…ë‹ˆë‹¤. ì •ìƒ ë²”ìœ„ëŠ” 150-200ë„ì¸ë° ìƒí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
    "max_tokens": 256
  }'

echo ""
echo "âœ… Deployment complete!"
echo "ğŸ“Š Server URL: http://localhost:8000"
echo "ğŸ¥ Health Check: http://localhost:8000/health"
echo "ğŸ“ˆ Stats: http://localhost:8000/api/v1/stats"
echo "ğŸ” Grafana: http://localhost:3000 (admin/admin)"

---
# benchmark.py
#!/usr/bin/env python3
"""
vLLM ì„œë²„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import aiohttp
import time
import json
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict

class VLLMBenchmark:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    async def single_request(self, session: aiohttp.ClientSession, request_data: dict) -> Dict:
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            async with session.post(
                f"{self.base_url}/api/v1/generate",
                json=request_data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                end_time = time.time()
                
                return {
                    "success": True,
                    "response_time": end_time - start_time,
                    "tokens_generated": result.get("tokens_generated", 0),
                    "status_code": response.status
                }
        except Exception as e:
            return {
                "success": False,
                "response_time": time.time() - start_time,
                "error": str(e),
                "tokens_generated": 0
            }
    
    async def concurrent_benchmark(self, concurrency: int, total_requests: int):
        """ë™ì‹œì„± ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ Running benchmark: {total_requests} requests with {concurrency} concurrency")
        
        # í…ŒìŠ¤íŠ¸ ìš”ì²­ ë°ì´í„°
        request_data = {
            "agent_type": "monitoring",
            "prompt": "ì œì¡° ë¼ì¸ì—ì„œ ì˜¨ë„ ì´ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì˜¨ë„ëŠ” 195ë„ì´ê³  ì •ìƒ ë²”ìœ„ëŠ” 150-200ë„ì…ë‹ˆë‹¤. ìƒí™©ì„ ë¶„ì„í•˜ê³  ê¶Œì¥ ì¡°ì¹˜ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
            "max_tokens": 256,
            "temperature": 0.1
        }
        
        start_time = time.time()
        
        async with aiohttp.ClientSession() as session:
            # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
            semaphore = asyncio.Semaphore(concurrency)
            
            async def limited_request():
                async with semaphore:
                    return await self.single_request(session, request_data)
            
            tasks = [limited_request() for _ in range(total_requests)]
            results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ê²°ê³¼ ë¶„ì„
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            tokens_generated = [r["tokens_generated"] for r in successful_requests]
            total_tokens = sum(tokens_generated)
            
            print(f"ğŸ“Š Results:")
            print(f"  âœ… Successful requests: {len(successful_requests)}/{total_requests}")
            print(f"  âŒ Failed requests: {len(failed_requests)}")
            print(f"  ğŸ• Total time: {total_time:.2f}s")
            print(f"  ğŸ“ˆ Requests/sec: {len(successful_requests)/total_time:.2f}")
            print(f"  ğŸ¯ Tokens/sec: {total_tokens/total_time:.2f}")
            print(f"  â±ï¸  Response time - Avg: {statistics.mean(response_times):.2f}s")
            print(f"  â±ï¸  Response time - P50: {statistics.median(response_times):.2f}s")
            print(f"  â±ï¸  Response time - P95: {sorted(response_times)[int(len(response_times)*0.95)]:.2f}s")
            print(f"  â±ï¸  Response time - P99: {sorted(response_times)[int(len(response_times)*0.99)]:.2f}s")
            
            # ê²°ê³¼ ì €ì¥
            self.results.append({
                "concurrency": concurrency,
                "total_requests": total_requests,
                "successful_requests": len(successful_requests),
                "requests_per_sec": len(successful_requests)/total_time,
                "tokens_per_sec": total_tokens/total_time,
                "avg_response_time": statistics.mean(response_times),
                "p95_response_time": sorted(response_times)[int(len(response_times)*0.95)],
                "total_time": total_time
            })
    
    async def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸ¯ vLLM Performance Benchmark Starting...")
        
        # ë‹¤ì–‘í•œ ë™ì‹œì„± ë ˆë²¨ í…ŒìŠ¤íŠ¸
        test_scenarios = [
            (1, 10),    # 1 concurrent, 10 requests
            (5, 50),    # 5 concurrent, 50 requests  
            (10, 100),  # 10 concurrent, 100 requests
            (20, 200),  # 20 concurrent, 200 requests
            (50, 500),  # 50 concurrent, 500 requests
        ]
        
        for concurrency, requests in test_scenarios:
            await self.concurrent_benchmark(concurrency, requests)
            await asyncio.sleep(2)  # ì„œë²„ íœ´ì‹ ì‹œê°„
        
        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“ˆ Benchmark Summary:")
        print("Concurrency | Requests | RPS   | Tokens/s | Avg RT | P95 RT")
        print("-" * 60)
        for result in self.results:
            print(f"{result['concurrency']:10d} | {result['total_requests']:8d} | "
                  f"{result['requests_per_sec']:5.1f} | {result['tokens_per_sec']:8.1f} | "
                  f"{result['avg_response_time']:6.2f} | {result['p95_response_time']:6.2f}")
        
        # JSON ê²°ê³¼ ì €ì¥
        with open("benchmark_results.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\nğŸ’¾ Results saved to benchmark_results.json")

if __name__ == "__main__":
    benchmark = VLLMBenchmark()
    asyncio.run(benchmark.run_full_benchmark())

---
# monitor.py
"""
vLLM ì„œë²„ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import aiohttp
import time
import json
from datetime import datetime

async def monitor_server(base_url: str = "http://localhost:8000"):
    """ì„œë²„ ëª¨ë‹ˆí„°ë§"""
    print("ğŸ” vLLM Server Monitoring Started...")
    
    async with aiohttp.ClientSession() as session:
        while True:
            try:
                # í—¬ìŠ¤ì²´í¬
                async with session.get(f"{base_url}/health") as response:
                    health_data = await response.json()
                
                # í†µê³„ ì¡°íšŒ
                async with session.get(f"{base_url}/api/v1/stats") as response:
                    stats_data = await response.json()
                
                # í˜„ì¬ ì‹œê°„
                now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # ëª¨ë‹ˆí„°ë§ ì •ë³´ ì¶œë ¥
                print(f"\n[{now}] Server Status:")
                print(f"  ğŸŸ¢ Status: {health_data['status']}")
                print(f"  ğŸ’¾ RAM Usage: {health_data['memory_usage']['ram_percent']:.1f}%")
                print(f"  ğŸ”¥ GPU Count: {health_data['gpu_count']}")
                print(f"  ğŸ“Š Active Requests: {health_data['active_requests']}")
                print(f"  ğŸ“ˆ Total Requests: {health_data['total_requests']}")
                print(f"  âš¡ Throughput: {health_data['throughput']:.2f} tokens/sec")
                print(f"  â° Uptime: {health_data['uptime']:.0f}s")
                
                if 'performance' in stats_data:
                    perf = stats_data['performance']
                    print(f"  ğŸ¯ Avg Tokens/Request: {perf['average_tokens_per_request']:.1f}")
                    print(f"  ğŸ“Š Total Tokens: {perf['total_tokens_generated']}")
                
            except Exception as e:
                print(f"âŒ Monitoring error: {e}")
            
            await asyncio.sleep(10)  # 10ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

if __name__ == "__main__":
    asyncio.run(monitor_server())