#!/usr/bin/env python3
"""
vLLM ì„œë²„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import aiohttp
import time
import json
import statistics
from typing import List, Dict


class VLLMBenchmark:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    async def single_request(self, session: aiohttp.ClientSession, request_data: dict) -> Dict:
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            async with session.post(
                f"{self.base_url}/api/v1/generate",
                json=request_data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                end_time = time.time()
                
                return {
                    "success": True,
                    "response_time": end_time - start_time,
                    "tokens_generated": result.get("tokens_generated", 0),
                    "status_code": response.status
                }
        except Exception as e:
            return {
                "success": False,
                "response_time": time.time() - start_time,
                "error": str(e),
                "tokens_generated": 0
            }
    
    async def concurrent_benchmark(self, concurrency: int, total_requests: int):
        """ë™ì‹œì„± ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ Running benchmark: {total_requests} requests with {concurrency} concurrency")
        
        # í…ŒìŠ¤íŠ¸ ìš”ì²­ ë°ì´í„°
        request_data = {
            "agent_type": "monitoring",
            "prompt": "ì œì¡° ë¼ì¸ì—ì„œ ì˜¨ë„ ì´ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì˜¨ë„ëŠ” 195ë„ì´ê³  ì •ìƒ ë²”ìœ„ëŠ” 150-200ë„ì…ë‹ˆë‹¤. ìƒí™©ì„ ë¶„ì„í•˜ê³  ê¶Œì¥ ì¡°ì¹˜ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
            "max_tokens": 256,
            "temperature": 0.1
        }
        
        start_time = time.time()
        
        async with aiohttp.ClientSession() as session:
            # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
            semaphore = asyncio.Semaphore(concurrency)
            
            async def limited_request():
                async with semaphore:
                    return await self.single_request(session, request_data)
            
            tasks = [limited_request() for _ in range(total_requests)]
            results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ê²°ê³¼ ë¶„ì„
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            tokens_generated = [r["tokens_generated"] for r in successful_requests]
            total_tokens = sum(tokens_generated)
            
            print(f"ğŸ“Š Results:")
            print(f"  âœ… Successful requests: {len(successful_requests)}/{total_requests}")
            print(f"  âŒ Failed requests: {len(failed_requests)}")
            print(f"  ğŸ• Total time: {total_time:.2f}s")
            print(f"  ğŸ“ˆ Requests/sec: {len(successful_requests)/total_time:.2f}")
            print(f"  ğŸ¯ Tokens/sec: {total_tokens/total_time:.2f}")
            print(f"  â±ï¸  Response time - Avg: {statistics.mean(response_times):.2f}s")
            print(f"  â±ï¸  Response time - P50: {statistics.median(response_times):.2f}s")
            print(f"  â±ï¸  Response time - P95: {sorted(response_times)[int(len(response_times)*0.95)]:.2f}s")
            print(f"  â±ï¸  Response time - P99: {sorted(response_times)[int(len(response_times)*0.99)]:.2f}s")
            
            # ê²°ê³¼ ì €ì¥
            self.results.append({
                "concurrency": concurrency,
                "total_requests": total_requests,
                "successful_requests": len(successful_requests),
                "requests_per_sec": len(successful_requests)/total_time,
                "tokens_per_sec": total_tokens/total_time,
                "avg_response_time": statistics.mean(response_times),
                "p95_response_time": sorted(response_times)[int(len(response_times)*0.95)],
                "total_time": total_time
            })
    
    async def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸ¯ vLLM Performance Benchmark Starting...")
        
        # ë‹¤ì–‘í•œ ë™ì‹œì„± ë ˆë²¨ í…ŒìŠ¤íŠ¸
        test_scenarios = [
            (1, 10),    # 1 concurrent, 10 requests
            (5, 50),    # 5 concurrent, 50 requests  
            (10, 100),  # 10 concurrent, 100 requests
            (20, 200),  # 20 concurrent, 200 requests
            (50, 500),  # 50 concurrent, 500 requests
        ]
        
        for concurrency, requests in test_scenarios:
            await self.concurrent_benchmark(concurrency, requests)
            await asyncio.sleep(2)  # ì„œë²„ íœ´ì‹ ì‹œê°„
        
        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“ˆ Benchmark Summary:")
        print("Concurrency | Requests | RPS   | Tokens/s | Avg RT | P95 RT")
        print("-" * 60)
        for result in self.results:
            print(f"{result['concurrency']:10d} | {result['total_requests']:8d} | "
                  f"{result['requests_per_sec']:5.1f} | {result['tokens_per_sec']:8.1f} | "
                  f"{result['avg_response_time']:6.2f} | {result['p95_response_time']:6.2f}")
        
        # JSON ê²°ê³¼ ì €ì¥
        with open("benchmark_results.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\nğŸ’¾ Results saved to benchmark_results.json")


if __name__ == "__main__":
    benchmark = VLLMBenchmark()
    asyncio.run(benchmark.run_full_benchmark()) 